{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Union, List\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from neuralop.data.datasets.pt_dataset import PTDataset\n",
    "from neuralop.data.datasets.web_utils import download_from_zenodo_record\n",
    "\n",
    "from neuralop.utils import get_project_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DarcyDataset(PTDataset):\n",
    "    \"\"\"\n",
    "    DarcyDataset stores data generated according to Darcy's Law.\n",
    "    Input is a coefficient function and outputs describe flow. \n",
    "\n",
    "    Data source: https://zenodo.org/records/12784353\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    train_db: torch.utils.data.Dataset of training examples\n",
    "    test_db:  \"\"                       of test examples\n",
    "    data_processor: neuralop.data.transforms.DataProcessor to process data examples\n",
    "        optional, default is None\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 root_dir: Union[Path, str],\n",
    "                 n_train: int,\n",
    "                 n_tests: List[int],\n",
    "                 batch_size: int,\n",
    "                 test_batch_sizes: List[int],\n",
    "                 train_resolution: int,\n",
    "                 test_resolutions: int=[16,32],\n",
    "                 encode_input: bool=False, \n",
    "                 encode_output: bool=True, \n",
    "                 encoding=\"channel-wise\",\n",
    "                 channel_dim=1,\n",
    "                 subsampling_rate=None,\n",
    "                 download: bool=True):\n",
    "\n",
    "        \"\"\"DarcyDataset\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        root_dir : Union[Path, str]\n",
    "            root at which to download data files\n",
    "        dataset_name : str\n",
    "            prefix of pt data files to store/access\n",
    "        n_train : int\n",
    "            number of train instances\n",
    "        n_tests : List[int]\n",
    "            number of test instances per test dataset\n",
    "        batch_size : int\n",
    "            batch size of training set\n",
    "        test_batch_sizes : List[int]\n",
    "            batch size of test sets\n",
    "        train_resolution : int\n",
    "            resolution of data for training set\n",
    "        test_resolutions : List[int], optional\n",
    "            resolution of data for testing sets, by default [16,32]\n",
    "        encode_input : bool, optional\n",
    "            whether to normalize inputs in provided DataProcessor,\n",
    "            by default False\n",
    "        encode_output : bool, optional\n",
    "            whether to normalize outputs in provided DataProcessor,\n",
    "            by default True\n",
    "        encoding : str, optional\n",
    "            parameter for input/output normalization. Whether\n",
    "            to normalize by channel (\"channel-wise\") or \n",
    "            by pixel (\"pixel-wise\"), default \"channel-wise\"\n",
    "        input_subsampling_rate : int or List[int], optional\n",
    "            rate at which to subsample each input dimension, by default None\n",
    "        output_subsampling_rate : int or List[int], optional\n",
    "            rate at which to subsample each output dimension, by default None\n",
    "        channel_dim : int, optional\n",
    "            dimension of saved tensors to index data channels, by default 1\n",
    "        \"\"\"\n",
    "\n",
    "        # convert root dir to Path\n",
    "        if isinstance(root_dir, str):\n",
    "            root_dir = Path(root_dir)\n",
    "        if not root_dir.exists():\n",
    "            root_dir.mkdir(parents=True)\n",
    "\n",
    "        # Zenodo record ID for Darcy-Flow dataset\n",
    "        zenodo_record_id = \"12784353\"\n",
    "\n",
    "        # List of resolutions needed for dataset object\n",
    "        resolutions = set(test_resolutions + [train_resolution])\n",
    "\n",
    "        # We store data at these resolutions on the Zenodo archive\n",
    "        available_resolutions = [16, 32, 64, 128, 421]\n",
    "        for res in resolutions:\n",
    "            assert res in available_resolutions, f\"Error: resolution {res} not available\"\n",
    "\n",
    "        # download darcy data from zenodo archive if passed\n",
    "        if download:\n",
    "            files_to_download = []\n",
    "            already_downloaded_files = [x for x in root_dir.iterdir()]\n",
    "            for res in resolutions:\n",
    "                if f\"darcy_train_{res}.pt\" not in already_downloaded_files or \\\n",
    "                f\"darcy_test_{res}.pt\" not in already_downloaded_files:    \n",
    "                    files_to_download.append(f\"darcy_{res}.tgz\")\n",
    "            download_from_zenodo_record(record_id=zenodo_record_id,\n",
    "                                        root=root_dir,\n",
    "                                        files_to_download=files_to_download)\n",
    "            \n",
    "        # once downloaded/if files already exist, init PTDataset\n",
    "        super().__init__(root_dir=root_dir,\n",
    "                       dataset_name=\"darcy\",\n",
    "                       n_train=n_train,\n",
    "                       n_tests=n_tests,\n",
    "                       batch_size=batch_size,\n",
    "                       test_batch_sizes=test_batch_sizes,\n",
    "                       train_resolution=train_resolution,\n",
    "                       test_resolutions=test_resolutions,\n",
    "                       encode_input=encode_input,\n",
    "                       encode_output=encode_output,\n",
    "                       encoding=encoding,\n",
    "                       channel_dim=channel_dim,\n",
    "                       input_subsampling_rate=subsampling_rate,\n",
    "                       output_subsampling_rate=subsampling_rate)\n",
    "\n",
    "\n",
    "        \n",
    "# legacy Small Darcy Flow example\n",
    "example_data_root = get_project_root() / \"neuralop/data/datasets/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_darcy_flow_small(n_train,\n",
    "    n_tests,\n",
    "    batch_size,\n",
    "    test_batch_sizes,\n",
    "    data_root = example_data_root,\n",
    "    test_resolutions=[16, 32],\n",
    "    encode_input=False,\n",
    "    encode_output=True,\n",
    "    encoding=\"channel-wise\",\n",
    "    channel_dim=1,):\n",
    "\n",
    "    dataset = DarcyDataset(root_dir = data_root,\n",
    "                           n_train=n_train,\n",
    "                           n_tests=n_tests,\n",
    "                           batch_size=batch_size,\n",
    "                           test_batch_sizes=test_batch_sizes,\n",
    "                           train_resolution=16,\n",
    "                           test_resolutions=test_resolutions,\n",
    "                           encode_input=encode_input,\n",
    "                           encode_output=encode_output,\n",
    "                           channel_dim=channel_dim,\n",
    "                           encoding=encoding,\n",
    "                           download=False)\n",
    "    \n",
    "    # return dataloaders for backwards compat\n",
    "    train_loader = DataLoader(dataset.train_db,\n",
    "                              batch_size=batch_size,\n",
    "                              num_workers=0,\n",
    "                              pin_memory=True,\n",
    "                              persistent_workers=False,)\n",
    "    \n",
    "    test_loaders = {}\n",
    "    for res,test_bsize in zip(test_resolutions, test_batch_sizes):\n",
    "        test_loaders[res] = DataLoader(dataset.test_dbs[res],\n",
    "                                       batch_size=test_bsize,\n",
    "                                       shuffle=False,\n",
    "                                       num_workers=0,\n",
    "                                       pin_memory=True,\n",
    "                                       persistent_workers=False,)\n",
    "    \n",
    "    return train_loader, test_loaders, dataset.data_processor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
